{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tundraden` examples\n",
    "\n",
    "## Generating correlated regressors and target\n",
    "\n",
    "Let's first generate some random correlated regressors. `generate_correlated_data` generates random data using a multivariate normal distribution. So by setting the covariance matrix one can decide how correlated the regressors are. Below I generate 3 regressors; the first 2 are strongly correlated to each other and the third one is uncorrelated to the 2 others. The regressors are then pair-plotted using the `pairplot` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tundraden\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "covariance_matrix = np.array([\n",
    "    [1.00, 1.00, 0.00],\n",
    "    [1.00, 1.01, 0.00],\n",
    "    [0.00, 0.00, 1.00],\n",
    "])\n",
    "X = tundraden.gen.generate_correlated_data(covariance_matrix, n_samples)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "tundraden.display.pairplot(X, var_names=['$x_1$', '$x_2$', '$x_3$'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the first 2 regressors are strongly correlated. The plots on the diagonal simply show the individual regressors distribution.\n",
    "\n",
    "Below I generate the target variable using arbitrary weights for each regressor. I also add some extra noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([-1, 2, 1])\n",
    "sigma = 0.5\n",
    "Y = tundraden.gen.generate_target(X, weights, sigma)\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.scatter(X[:,i], Y)\n",
    "    plt.xlabel('$x_%i$'%(i+1))\n",
    "    plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS linear regression\n",
    "\n",
    "The simplest model is an OLS linear regression and can be performed with the `LinearRegression` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tundraden.models.LinearRegression()\n",
    "model.fit(X, Y)\n",
    "model.coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the run the estimated weights above can be quite poor.\n",
    "\n",
    "A way of measuring the multicollinearity of the data is to calculate the coefficient of multiple determination and/or the variance inflation factor of each of the predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tundraden.metrics.partial_r2(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tundraden.metrics.partial_vif(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen both the partial coefficients of determination and the variance inflation ratios are high for the first two regressors.\n",
    "\n",
    "## Ridge regression and hyper-parameter tuning\n",
    "\n",
    "### Simple train-test split\n",
    "\n",
    "To deal with multicollinearity, one can consider a Ridge regression model. The RidgeRegression class can do Ridge regression. To chose the optimal hyper-parameter $\\lambda$, below I first do a naive hold-out split. The optimal $\\lambda$ value is the one giving the lowest error on the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = tundraden.cv.train_test_split(X, Y, train_size=0.67)\n",
    "testing_errors = np.zeros(len(lambdas))\n",
    "for i, lambda_ in enumerate(lambdas):\n",
    "    model = tundraden.models.RidgeRegression(lambda_)\n",
    "    model.fit(X_train, Y_train)\n",
    "    testing_errors[i] = model.test(X_test, Y_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lambdas, testing_errors)\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('Testing error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the split into training and testing sets is done randomly, the code snippet above can give different results if run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = tundraden.cv.train_test_split(X, Y, train_size=0.67)\n",
    "testing_errors = np.zeros(len(lambdas))\n",
    "for i, lambda_ in enumerate(lambdas):\n",
    "    model = tundraden.models.RidgeRegression(lambda_)\n",
    "    model.fit(X_train, Y_train)\n",
    "    testing_errors[i] = model.test(X_test, Y_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lambdas, testing_errors)\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('Testing error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold cross-validation\n",
    "\n",
    "To deal with this on can do K-fold cross-validation. The `RidgeCV` manages the hyper-parameter tuning using cross-validation. Below I use 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCV = tundraden.models.RidgeCV(cv=10)\n",
    "modelCV.fit(X, Y)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(modelCV.lambda_grid, modelCV.generalization_errors)\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('Generalization error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal $\\lambda$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCV.lambda_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-one-out cross-validation\n",
    "\n",
    "I can also do leave-one-out cross-validation, which is the default behavior of the `RidgeCV` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCV = tundraden.RidgeCV()\n",
    "modelCV.fit(X, Y)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(modelCV.lambda_grid, modelCV.generalization_errors)\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('Generalization error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the optimal $\\lambda$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelCV.lambda_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-level cross-validation\n",
    "\n",
    "The generalization error of a fitted `RidgeCV` object is a biased estimate of the performance of the model. To get an unbiased estimate of the generalization error, two-level cross-validation is prefered. The `NestedRidgeCV` class can perform two-level cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nestedCV = tundraden.NestedRidgeCV(outer_cv=5, inner_cv=10)\n",
    "nestedCV.run(X, Y)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(nestedCV.test_errors, 'x')\n",
    "plt.xlabel('Model index')\n",
    "plt.ylabel('Test error')\n",
    "plt.xticks(np.arange(len(nestedCV.test_errors)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `run` method of a `NestedRidgeCV` object trains as many `RidgeCV` models as outer-folds. In this case we have 5 models. The `models` attribute contains all the trained models. The optimal hyper-parameter and the weights of each model can easily be retreived:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nestedCV.models[3].coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nestedCV.models[3].lambda_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall generalization error is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nestedCV.generalization_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyper-parameters of all the models can be directly retreived with the `lambdas` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nestedCV.lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here all the models have the same $\\lambda$, but it's not usually the case!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
